{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义张量的形状\n",
    "tensor_shape = (10, 4)\n",
    "\n",
    "# 创建指定形状的零张量\n",
    "zero_tensor = torch.zeros(tensor_shape)\n",
    "\n",
    "# 定义索引数组\n",
    "indices = [0, 1, 2, 2, 3, 3, 3, 3, 3, 3]\n",
    "\n",
    "# 将对应的值设置为1\n",
    "for i, index in enumerate(indices):\n",
    "    zero_tensor[i, index] = 1\n",
    "\n",
    "# 输出结果张量\n",
    "print(zero_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_nan: tensor(False)\n",
      "torch.Size([64, 25])\n",
      "torch.Size([1600, 1])\n",
      "tensor([2.9053, 2.2970, 2.2119, 2.0351, 2.0200, 1.6500, 1.6300, 1.4243, 1.2400,\n",
      "        1.1856])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.load(\"tensor_s.pt\")\n",
    "\n",
    "model = torch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Beta, Normal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class Actor_Gaussian(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Actor_Gaussian, self).__init__()\n",
    "        self.max_action = args.max_action\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_width)\n",
    "        self.fc2 = nn.Linear(args.hidden_width, args.hidden_width)\n",
    "        self.mean_layer = nn.Linear(args.hidden_width, args.continuous_action_dim)\n",
    "        self.log_std = nn.Parameter(\n",
    "            torch.zeros(1, args.continuous_action_dim))  # We use 'nn.Parameter' to train log_std automatically\n",
    "        self.activate_func = [nn.ReLU(), nn.Tanh()][args.use_tanh]  # Trick10: use tanh\n",
    "        self.discrete_action_fc = nn.Linear(args.hidden_width, args.discrete_action_dim)\n",
    "\n",
    "        if args.use_orthogonal_init:\n",
    "            print(\"------use_orthogonal_init------\")\n",
    "            orthogonal_init(self.fc1)\n",
    "            orthogonal_init(self.fc2)\n",
    "            orthogonal_init(self.mean_layer, gain=0.01)\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = self.activate_func(self.fc1(s))\n",
    "        s = self.activate_func(self.fc2(s))\n",
    "        mean = (torch.tanh(self.mean_layer(s)) + 1) / 2  # [-1,1]->[-max_action,max_action]\n",
    "        a_prob = torch.softmax(self.discrete_action_fc(s), dim=1)\n",
    "        # mean[:, 0] *= 100\n",
    "        # mean[:, 1] *= 10000\n",
    "\n",
    "        return mean, a_prob\n",
    "\n",
    "    def get_dist(self, s):\n",
    "        mean, prob = self.forward(s)\n",
    "        # print(\"mean_data:\",mean)\n",
    "        log_std = self.log_std.expand_as(mean)  # To make 'log_std' have the same dimension as 'mean'\n",
    "        std = torch.exp(log_std)  # The reason we train the 'log_std' is to ensure std=exp(log_std)>0\n",
    "        try:\n",
    "            dist = Normal(mean, std)  # Get the Gaussian distribution\n",
    "        except:\n",
    "            print(\"s:\", s, \"mean:\", mean, \"std:\", std)\n",
    "            torch.save(s, \"tensor_s.pt\")\n",
    "            torch.save(self.state_dict(), \"model_error/actor.pkl\")\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([64, 25])\n",
      "tensor([[ 0.3806,  0.2347,  0.0380,  ...,  0.1200,  0.2990,  0.3036],\n",
      "        [ 0.2724, -0.9437, -0.3541,  ...,  0.0900,  0.2330,  0.3341],\n",
      "        [ 0.0813,  0.3935, -0.0339,  ...,  0.3200,  0.2990,  0.3341],\n",
      "        ...,\n",
      "        [ 0.2150, -0.0887, -0.2255,  ...,  0.0100,  0.2990,  0.3341],\n",
      "        [ 0.2751, -0.7553, -0.8865,  ...,  0.5000,  0.2180,  0.3341],\n",
      "        [ 0.4690, -2.3496, -1.2436,  ...,  0.0100,  0.1430,  0.3341]])\n",
      "(tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<DivBackward0>), tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan],\n",
      "        [nan, nan, nan]], grad_fn=<SoftmaxBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Beta, Normal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "data = torch.load(\"tensor_s.pt\")\n",
    "print(\"shape:\",data.shape)\n",
    "actor_net =  torch.load(\"model_error/actor.pkl\")\n",
    "actor_net.eval()\n",
    "\n",
    "self_input_data = torch.full((64,25),0.1)\n",
    "\n",
    "print(data)\n",
    "\n",
    "out = actor_net(self_input_data)\n",
    "\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
